{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COIaQep43Zpx"
   },
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24Auozc7BcT2",
    "outputId": "26ee53aa-cc57-41ce-97c7-9ca177560548"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ7bUQbtB_b2"
   },
   "source": [
    "# Mount drive - if you are using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-hQWh7Tij-6",
    "outputId": "41dfc153-a557-42dc-8e95-13f9468dc4aa"
   },
   "outputs": [],
   "source": [
    "# choose where you want to run the model\n",
    "run = 'local' # colab or local\n",
    "if run == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7qEA26wCDvV"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "dHauAb2HB8Lb",
    "outputId": "13a0c35d-e5e2-453f-de22-f4a5b06d3d8c"
   },
   "outputs": [],
   "source": [
    "# Define path\n",
    "if run == 'colab':\n",
    "    base = '/content/drive/My Drive/Hatefulle Ytringer Models/'\n",
    "else:\n",
    "    base = './'\n",
    "\n",
    "# Load data\n",
    "unlabel = pd.read_csv(base+'data/data.csv', encoding = \"UTF-8\")\n",
    "unlabel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ANGuFr66B8O0",
    "outputId": "9485c517-58f5-44c7-b5b3-1a74bcf92007"
   },
   "outputs": [],
   "source": [
    "# drop answer column\n",
    "unlabel = unlabel[['post_id', 'comment_id', 'author_id', 'text']]\n",
    "\n",
    "# drop null rows\n",
    "unlabel = unlabel.dropna()\n",
    "unlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iZFbigkVUVem"
   },
   "outputs": [],
   "source": [
    "def process_tweet(df):\n",
    "    '''\n",
    "    Input: \n",
    "        df: a dataframe containing a column 'text' of strings of tweets\n",
    "    Output:\n",
    "        df with a column 'tweets_clean'\n",
    "    \n",
    "    '''\n",
    "    #remove URL\n",
    "    df['tweet_proc'] = df['text'].str.replace(r'http(\\S)+', r'')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'http ...', r'')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'http', r'')\n",
    "    df[df['tweet_proc'].str.contains(r'http')]\n",
    "\n",
    "    # remove RT, @\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "    df[df['tweet_proc'].str.contains(r'RT[ ]?@')]\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'@[\\S]+',r'')\n",
    "\n",
    "    #remove &, < og >\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'&amp;?',r'og')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'&lt;',r'<')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'&gt;',r'>')\n",
    "\n",
    "    # remove extra space\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'[ ]{2, }',r' ')\n",
    "\n",
    "    # insert space between punctuation marks\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "\n",
    "    # lower case and strip white spaces at both ends\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.lower()\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFO-urYBWiqy"
   },
   "source": [
    "# Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "iPrmr9zFQTaD"
   },
   "outputs": [],
   "source": [
    "unlabel = process_tweet(unlabel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGkRvRBXWlWf"
   },
   "source": [
    "# Convert emoji into words and remove non-alphabetic characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eHhqLu6yDrR5"
   },
   "outputs": [],
   "source": [
    "unlabel['tweet_proc'] = unlabel['tweet_proc'].str.replace(':-\\)', 'smile')\n",
    "unlabel['tweet_proc'] = unlabel['tweet_proc'].str.replace(':-\\(', 'trist')\n",
    "unlabel['tweet_proc'] = unlabel['tweet_proc'].str.replace(r'[^a-zåøæ ]', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S93DPGFeXD-3"
   },
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3ZyriRCgUoA2"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('norwegian')\n",
    "stop_words.remove('ikke')\n",
    "stop_words.remove('ikkje')\n",
    "\n",
    "unlabel['tweet_proc'] = unlabel['tweet_proc'].apply(lambda x:' '.join(w for w in x.split() if w not in stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONPiCHCcXHKT"
   },
   "source": [
    "# Load trained tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s43yRj-MKlI_",
    "outputId": "2f7d0611-6756-4f16-a4d4-388400ad790d"
   },
   "outputs": [],
   "source": [
    "with open(base +'tokenizer/tokenizer.json_16102021_v1') as f:\n",
    "    data = json.load(f)\n",
    "    tokenizer_trained = tokenizer_from_json(data)\n",
    "\n",
    "    vocab_size = len(tokenizer_trained.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "print('vocab_size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFbl4jpeXN0v"
   },
   "source": [
    "# Convert words into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "t40VWNxsF0eH"
   },
   "outputs": [],
   "source": [
    "unseen = tokenizer_trained.texts_to_sequences(unlabel['tweet_proc'].values)\n",
    "unseen = pad_sequences(unseen, maxlen=128, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O3dSOCvYXb4f"
   },
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "if4XzkWTEO_z"
   },
   "outputs": [],
   "source": [
    "def create_ann_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 28))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = create_ann_model()\n",
    "\n",
    "checkpoint_filepath = base+'model/final_model.hdf5'\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slwR44u4Xeap"
   },
   "source": [
    "# Classify unlabeled data using trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "CT1Gx5mNE-7q",
    "outputId": "4aa6c911-ced8-4b27-95b0-772a66b1fcca"
   },
   "outputs": [],
   "source": [
    "yhat = model.predict(unseen)\n",
    "yhat = [1 if y>0.55 else 0 for y in yhat]\n",
    "unlabel['Result'] = yhat\n",
    "unlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EySNAawJFXQQ",
    "outputId": "7a38035f-e38a-474d-a8f8-4eb6c516d497"
   },
   "outputs": [],
   "source": [
    "# Number of cases in predicted in each class\n",
    "unlabel['Result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqfDJ0FMGKkT"
   },
   "outputs": [],
   "source": [
    "# save the result \n",
    "unlabel.to_csv(base+'data/data_classified.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "unseen data classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
