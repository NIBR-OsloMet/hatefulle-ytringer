{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COIaQep43Zpx"
   },
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24Auozc7BcT2",
    "outputId": "ff1b55bd-ac8e-4303-f4b9-cd6ed9874f8f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ7bUQbtB_b2"
   },
   "source": [
    "# Mount drive - if you are using Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q-hQWh7Tij-6",
    "outputId": "1a7e860d-1745-4250-c090-98869ad45668"
   },
   "outputs": [],
   "source": [
    "run = 'local' # local, colab\n",
    "if run == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7qEA26wCDvV"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JeTKPwrFvGSL"
   },
   "outputs": [],
   "source": [
    "# Define folder path \n",
    "if run == 'colab':\n",
    "    path = '/content/drive/My Drive/Hatefulle Ytringer Models/'\n",
    "else:\n",
    "    path = './'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "hD8wtVa8BvRX",
    "outputId": "e423c667-123e-4200-aafa-101717b5c755"
   },
   "outputs": [],
   "source": [
    "# Read data file\n",
    "Text_df = pd.read_csv(path+'data/tweets_data.csv', sep=',', usecols=['text', 'classification']  ,encoding = \"UTF-8\")\n",
    "Text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79odQrq0CGJ4"
   },
   "source": [
    "# Check data records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hUKI8zIB7ko",
    "outputId": "ef7da982-657c-4297-e987-73ea2e90a6fc"
   },
   "outputs": [],
   "source": [
    "# Number of cases in each class\n",
    "Text_df['classification'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jw70FJA74VSD"
   },
   "source": [
    "# Define function to preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M-uQb3tmCWEo"
   },
   "outputs": [],
   "source": [
    "def process_tweet(df):\n",
    "    '''\n",
    "    Input: \n",
    "        df: a dataframe containing a column 'text' of strings of tweets\n",
    "    Output:\n",
    "        df with a column 'tweets_clean'\n",
    "    \n",
    "    '''\n",
    "    #remove URL\n",
    "    df['tweet_proc'] = df['text'].str.replace(r'http(\\S)+', r'')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'http ...', r'')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'http', r'')\n",
    "    df[df['tweet_proc'].str.contains(r'http')]\n",
    "\n",
    "    # remove RT, @\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'(RT|rt)[ ]*@[ ]*[\\S]+',r'')\n",
    "    df[df['tweet_proc'].str.contains(r'RT[ ]?@')]\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'@[\\S]+',r'')\n",
    "\n",
    "    #remove &, < og >\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'&amp;?',r'og')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'&lt;',r'<')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'&gt;',r'>')\n",
    "\n",
    "    # remove extra space\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'[ ]{2, }',r' ')\n",
    "\n",
    "    # insert space between punctuation marks\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'([\\w\\d]+)([^\\w\\d ]+)', r'\\1 \\2')\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.replace(r'([^\\w\\d ]+)([\\w\\d]+)', r'\\1 \\2')\n",
    "\n",
    "    # lower case and strip white spaces at both ends\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.lower()\n",
    "    df['tweet_proc'] = df['tweet_proc'].str.strip()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "0vPZX-zzCeO7",
    "outputId": "5d048ede-db6d-400d-828e-1d69213c1f01"
   },
   "outputs": [],
   "source": [
    "# clean text\n",
    "Text_df = process_tweet(Text_df)\n",
    "Text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NREel77ICwgK"
   },
   "source": [
    "# List the most common and least common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ghZmoa9Cvb1",
    "outputId": "03e76921-a2ac-4264-b310-a9b079d01cac"
   },
   "outputs": [],
   "source": [
    "data = Counter(''.join(Text_df['tweet_proc'].values).split())\n",
    "\n",
    "print('Most occurance word: \\n\\n', data.most_common()[:10])\n",
    "print('\\n\\nLeast occurance word: \\n\\n', data.most_common()[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAGL-U4T6oYI"
   },
   "source": [
    "We can oberve that the dataset contains a lots of non-alphabetic words. These words does not add value in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zlDTnOQbEjnl"
   },
   "source": [
    "# List the number of words with single occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ix9QxxbCDX0g",
    "outputId": "532611da-66f9-40f6-d908-8543f8041787"
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "for word, num in data.items():\n",
    "    if num==1:\n",
    "        count+=1\n",
    "print('{} number of words occur only one time'.format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WwhdOvw9Ewt"
   },
   "source": [
    "# Further cleaning of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Ft1mRWDkvZZ7"
   },
   "outputs": [],
   "source": [
    "# convert smile emoji to smile word\n",
    "Text_df['tweet_proc'] = Text_df['tweet_proc'].str.replace(':-\\)', 'smile')\n",
    "\n",
    "# convert sad emoji to sad word\n",
    "Text_df['tweet_proc'] = Text_df['tweet_proc'].str.replace(':-\\(', 'trist')\n",
    "\n",
    "# remove all non-alphabetic characters\n",
    "Text_df['tweet_proc'] = Text_df['tweet_proc'].str.replace(r'[^a-zåøæ ]', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XzPbOYQ9W_P"
   },
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9z7EK0CR9ViK"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('norwegian')\n",
    "stop_words.remove('ikke')\n",
    "stop_words.remove('ikkje')\n",
    "\n",
    "Text_df['tweet_proc'] = Text_df['tweet_proc'].apply(lambda x:' '.join(w for w in x.split() if w not in stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VelDXONBEk-u"
   },
   "source": [
    "# Remove comments that are shorter than four words after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "MAPvu8y8zGZb",
    "outputId": "4edfe279-a72d-4ed9-e6f9-83375c4a2d7a"
   },
   "outputs": [],
   "source": [
    "Text_df = Text_df[Text_df.tweet_proc.apply(lambda x: len(x.split())>3)]\n",
    "Text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikBqpzjydgcy"
   },
   "source": [
    "# Count total number of words (vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2LnGhc_Pzudj",
    "outputId": "603f6eac-9f1a-4444-9c20-8b488dd186c2"
   },
   "outputs": [],
   "source": [
    "vocab_length = len(Counter(' '.join(Text_df['tweet_proc'].values).split()))\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKSVyYFnHNo1"
   },
   "source": [
    "# Convert vocabulary into tokenizer and sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UB3_nrIJgApq",
    "outputId": "0ef8c221-50d1-4f52-8622-313274d5d26d"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_length)\n",
    "tokenizer.fit_on_texts(Text_df['tweet_proc'].values)\n",
    "\n",
    "X = tokenizer.texts_to_sequences(Text_df['tweet_proc'].values)\n",
    "X = pad_sequences(X, maxlen=128, padding='post', truncating='post')\n",
    "Y = Text_df['classification']\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "print('vocab_size: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_YQyMGEHsDd"
   },
   "source": [
    "# Save tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "T8cdJDKrJGzx"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "tokenizer_json_2 = tokenizer.to_json()\n",
    "with io.open(path + 'tokenizer/tokenizer.json_16102021_v1', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json_2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5OqgNmeIAxQ"
   },
   "source": [
    "# Split the data into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "D4xAzUjciTpS"
   },
   "outputs": [],
   "source": [
    "X_train, X2, y_train, y2 = train_test_split(X, Y, test_size=0.3, random_state=42, stratify=Y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X2, y2, test_size=0.5, stratify=y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8h5tWYYILNR"
   },
   "source": [
    "# replace 99 () as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0z_55ETojNqY"
   },
   "outputs": [],
   "source": [
    "y_train = y_train.replace(99, 1)\n",
    "y_val = y_val.replace(99, 1)\n",
    "y_test = y_test.replace(99, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZVWROgFIZU8"
   },
   "source": [
    "# Count number of cases used in train, test and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVRgaJxpjrRf",
    "outputId": "d14615b5-6a11-4263-bf81-f6310a3e3656"
   },
   "outputs": [],
   "source": [
    "print(f'Train data shape {X_train.shape}, {y_train.shape}, {Counter(y_train)}')\n",
    "print(f'Validation data shape {X_val.shape}, {y_val.shape}, {Counter(y_val)}')\n",
    "print(f'Test data shape {X_test.shape}, {y_test.shape}, {Counter(y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lh-m69oCIr6g"
   },
   "source": [
    "# Build  an Artificial Neural Network (ANN) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NSi_m74wj2bG"
   },
   "outputs": [],
   "source": [
    "def create_ann_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 28))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_ann_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 32))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nTZWWQbqI6t5"
   },
   "source": [
    "# define checkpoint to store model that yields highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ebwdoianG-jh"
   },
   "outputs": [],
   "source": [
    "# create folder to store current model\n",
    "dt = datetime.datetime.now()\n",
    "dt = str(dt).replace(' ', '_')[:-7]\n",
    "\n",
    "if not os.path.exists(path+'checkpoint/'+f'model_{dt}'):\n",
    "    os.mkdir(path+'checkpoint/'+f'model_{dt}')\n",
    "else:\n",
    "    for m in os.listdir(path+'checkpoint/'+f'model_{dt}/'):\n",
    "        os.remove(path+'checkpoint/'+f'model_{dt}/'+m)\n",
    "\n",
    "checkpoint_filepath = path+f'checkpoint/model_{dt}/'+'ANN-{epoch:02d}-{val_accuracy:.4f}.hdf5'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcryWbIUlTYW",
    "outputId": "19d56b2f-c5db-4522-d16c-3be7859a6eea"
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "ann = create_ann_model()\n",
    "# ann = create_ann_model_2()\n",
    "\n",
    "# train and validate with default model\n",
    "ann.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=15, verbose=0, batch_size=10, callbacks=[model_checkpoint_callback])\n",
    "_, acc = ann.evaluate(X_test, y_test)\n",
    "print('Test accuracy of default model: ', acc)\n",
    "\n",
    "# findout best model path from saved folder \n",
    "acc = 0\n",
    "best_model = None\n",
    "for mpath in os.listdir(path+f'checkpoint/model_{dt}/'):\n",
    "    val = float(mpath.split('-')[-1].replace('.hdf5',''))\n",
    "    if val>acc:\n",
    "        best_model = mpath\n",
    "\n",
    "# load the best model\n",
    "checkpoint_filepath = path+f'checkpoint/model_{dt}/'+best_model\n",
    "ann.load_weights(checkpoint_filepath)\n",
    "\n",
    "# test the model\n",
    "yhat_prob = ann.predict(X_test)\n",
    "_, acc = ann.evaluate(X_test, y_test)\n",
    "print('Test accuracy of saved model: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTp8YuWLaKM3"
   },
   "source": [
    "# Set the probability treshold for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "xKuIs2QOwuck"
   },
   "outputs": [],
   "source": [
    "yhat = [1 if y>0.5 else 0 for y in yhat_prob]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvEPI3ELak-d"
   },
   "source": [
    "# Create function to print model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "f29my1XoxveQ"
   },
   "outputs": [],
   "source": [
    "def print_metrics(real, predict):\n",
    "    print('Accuracy: ', accuracy_score(real, predict))\n",
    "    print('\\nPrecision: ', precision_score(real, predict))\n",
    "    print('\\nrecall: ', recall_score(real, predict))\n",
    "    print('\\nf1_score: ', f1_score(real, predict))\n",
    "    print('\\nconfusion_matrix:\\n ', pd.DataFrame(confusion_matrix(real, predict), index=[0, 1], columns=[0, 1]))\n",
    "    print('\\nclassification_report:\\n ', classification_report(real, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_gwS6atysh-",
    "outputId": "61725d2e-7329-4080-bbb7-1db6c6dcd3a0"
   },
   "outputs": [],
   "source": [
    "print_metrics(y_test, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1D6VFajbQxX"
   },
   "source": [
    "# AUC ROC curve\n",
    "<br>\n",
    "\n",
    "True Positive Rate = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "False Positive Rate = False Positives / (False Positives + True Negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "THjOQrSgnqnJ",
    "outputId": "28135528-da9b-4c39-94f3-c57f19ae700e"
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr):\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "y_score = np.array(yhat_prob)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "print('ROC_AUC Score: ', roc_auc_score(y_test, y_score))\n",
    "\n",
    "# find optimal threshold\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Optimal threshold value is:\", optimal_threshold)\n",
    "plot_roc_curve(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F83XpZrnb_7V"
   },
   "source": [
    "# Model accuracy based on optimal threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HP8TO8l-nqxN",
    "outputId": "7e26c2c2-17a1-4f3f-ff2c-35897e4d268e"
   },
   "outputs": [],
   "source": [
    "yhat = [1 if y>optimal_threshold else 0 for y in yhat_prob]\n",
    "print_metrics(y_test, yhat)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Hatefulle Ytringer model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
